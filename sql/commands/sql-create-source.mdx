---
title: "CREATE SOURCE"
description: "A source is a resource that RisingWave can read data from. You can create a source in RisingWave using the `CREATE SOURCE` command."
---

For the full list of the sources we support, see [Supported sources](/ingestion/overview#supported-sources).

If you choose to persist the data from the source in RisingWave, use the [CREATE TABLE](/sql/commands/sql-create-table) command with connector settings. For most CDC formats (FORMAT DEBEZIUM, MAXWELL, CANAL), you must use `CREATE TABLE` to properly handle updates and deletes. For more details about the differences between sources and tables, see [here](/ingestion/overview#table-with-connectors).

Regardless of whether the data is persisted in RisingWave, you can create materialized views to perform analysis or data transformations.

## Syntax

```sql
CREATE SOURCE [ IF NOT EXISTS ] source_name (
    [PRIMARY KEY (col_name, ...)],
    col_name data_type [ AS generation_expression ],
    ...
   [ watermark_clause ]
)
[INCLUDE { header | key | offset | partition | timestamp } [AS <column_name>]]
[ WITH (
    connector='connector_name',
    connector_parameter='value', ...)]
[FORMAT data_format ENCODE data_encode [ (
    message='message',
    schema.location='location', ...) ]
];
```

## Notes

A [generated column](/sql/query-syntax/generated-columns) is defined with non-deterministic functions. When the data is ingested, the function will be evaluated to generate the value of this field.

Names and unquoted identifiers are case-insensitive. Therefore, you must double-quote any of these fields for them to be case-sensitive. See also [Identifiers](/sql/identifiers).

To know when a data record is loaded to RisingWave, you can define a column that is generated based on the processing time (`<column_name> timestamptz AS proctime()`) when creating the table or source. See also [proctime()](/sql/functions/datetime#proctime).

For a source with schema from an external connector, use `*` to represent all columns from the external connector first, so that you can define a generated column on the source with an external connector. See the example below.

```js
CREATE SOURCE from_kafka (
  *,
  gen_i32_field INT AS int32_field + 2
)
INCLUDE KEY AS some_key
WITH (
  connector = 'kafka',
  topic = 'test-rw-sink-upsert-avro',
  properties.bootstrap.server = 'message_queue:29092'
)
FORMAT upsert ENCODE AVRO (
  schema.registry = 'http://message_queue:8081'
);
```

<Note>
The generated column is created in RisingWave and will not be accessed through the external connector. Therefore, if the external upstream system has a schema, it does not need to include the generated column within the table's schema in the external system.
</Note>

## Parameter

| Parameter                         | Description                                                                                                                                                                                                                                                                                  |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| _source\_name_                    | The name of the source. If a schema name is given (for example, CREATE SOURCE \<schema>.\<source> ...), then the table is created in the specified schema. Otherwise it is created in the current schema.                                                                                      |
| _col\_name_                       | The name of a column.                                                                                                                                                                                                                                                                        |
| _data\_type_                      | The data type of a column. With the struct data type, you can create a nested table. Elements in a nested table need to be enclosed with angle brackets (\<>).                                                                                                                                |
| _generation\_expression_          | The expression for the generated column. For details about generated columns, see [Generated columns](/sql/query-syntax/generated-columns).                                                                                                                                        |
| _watermark\_clause_               | A clause that defines the watermark for a timestamp column. The syntax is WATERMARK FOR column\_name as expr. For details about watermarks, refer to [Watermarks](/processing/watermarks).                                                                                                |
| **INCLUDE** clause                | Extract fields not included in the payload as separate columns. For more details on its usage, see [INCLUDE clause](/ingestion/extract-metadata-from-sources).                                                                                                                                          |
| **WITH** clause                   | Specify the connector settings here if trying to store all the source data. See [Supported sources](/ingestion/overview#supported-sources) for the full list of supported source as well as links to specific connector pages detailing the syntax for each source. |
| **FORMAT** and **ENCODE** options | Specify the data format and the encoding format of the source data. To learn about the supported data formats, see [Data formats and encoding options](/ingestion/formats-and-encoding-options).                                                                                      |

<Note>
Please distinguish between the parameters set in the FORMAT and ENCODE options and those set in the WITH clause. Ensure that you place them correctly and avoid any misuse.
</Note>

## Connector-specific FORMAT requirements

Different connectors have specific FORMAT and ENCODE requirements:

- **CDC connectors** (e.g., `mysql-cdc`, `postgres-cdc`): Must use `FORMAT DEBEZIUM ENCODE JSON` (or `FORMAT PLAIN ENCODE JSON` for CDC source jobs). The FORMAT clause can be omitted as it defaults to the required format.

- **Nexmark connector**: Must use `FORMAT NATIVE ENCODE NATIVE`. The FORMAT clause can be omitted.

- **Datagen connector**: Defaults to `FORMAT NATIVE ENCODE NATIVE`, but other formats can be specified.

- **Iceberg connector**: Uses `FORMAT NONE ENCODE NONE` (self-describing format). The FORMAT clause can be omitted.

- **Other connectors** (Kafka, Kinesis, Pulsar, etc.): Require explicit FORMAT and ENCODE specification based on the actual data format.

If you specify an incompatible FORMAT for a connector, you will receive a parser error indicating the expected format.

<Note>
NOT NULL constraints are not supported in source schemas. If you need NOT NULL enforcement, use `CREATE TABLE` with a connector instead.
</Note>
## Watermarks

RisingWave supports generating watermarks when creating a source. Watermarks are like markers or signals that track the progress of event time, allowing you to process events within their corresponding time windows. The [WATERMARK](/processing/watermarks) clause should be used within the `schema_definition`. For more information on how to create a watermark, see [Watermarks](/processing/watermarks).

## Upsert sources

You can create sources with `FORMAT UPSERT` to handle streams with updates and deletes based on a primary key. When using `FORMAT UPSERT`, you must:

- Include the `INCLUDE KEY` clause to specify a key column
- Define a `PRIMARY KEY` using the key column

```sql Example
CREATE SOURCE upsert_source (
    PRIMARY KEY (rw_key),
    id INT,
    name VARCHAR,
    age INT
)
INCLUDE KEY AS rw_key
WITH (
    connector='kafka',
    topic='user_updates',
    properties.bootstrap.server='localhost:9092'
)
FORMAT UPSERT ENCODE JSON;
```

**Recommended usage patterns**

1. **Simple ETL with filtering and transformation:**
   ```sql
   CREATE SINK filtered_sink INTO target_table AS
   SELECT 
       id,
       UPPER(name) AS name,
       age
   FROM upsert_source
   WHERE age >= 18;
   ```

2. **Materialize to enable stateful operations:**
    ```sql
    -- Create a materialized view to hold a clean subset of data
    CREATE MATERIALIZED VIEW valid_users_mv AS
    SELECT 
        id, 
        LOWER(email) AS email,
        age,
        country_code
    FROM upsert_source
    WHERE email IS NOT NULL
      AND age >= 18;

    -- Now you can perform aggregations on the cleaned state
    SELECT country_code, COUNT(*) 
    FROM valid_users_mv 
    GROUP BY country_code;
    ```

<Note>
Upsert sources emit **upsert streams**, which do not contain old values for `UPDATE` operations. Therefore, stateful operators like JOINs, aggregations, and window functions do not work on an upsert source. Attempting to do so will result in errors like `"upsert stream is not supported as input of ..."`.
</Note>

## Change Data Capture (CDC)

Change Data Capture (CDC) refers to the process of identifying and capturing data changes in a database, and then delivering the changes to a downstream service in real-time.

RisingWave provides native MySQL and PostgreSQL CDC connectors. With these CDC connectors, you can ingest CDC data from these databases directly, without setting up additional services like Kafka.

If Kafka is part of your technical stack, you can also use the Kafka connector in RisingWave to ingest CDC data in the form of Kafka topics from databases into RisingWave. You need to use a CDC tool such as [Debezium connector for MySQL](https://debezium.io/documentation/reference/stable/connectors/mysql.html) or [Maxwell's daemon](https://maxwells-daemon.io/) to convert CDC data into Kafka topics.

For complete step-to-step guides about ingesting MySQL and PostgreSQL data using both approaches, see [Ingest data from MySQL](/ingestion/sources/mysql/mysql-cdc) and [Ingest data from PostgreSQL](/integrations/sources/postgresql-cdc).

## Shared source

Shared source improves resource utilization and data consistency when working with Kafka sources in RisingWave. This will only affect Kafka sources created after the version updated and will not affect any existing Kafka sources.

<Note>
Shared Kafka source is available since version 2.1. Other sources are unaffected. We plan to gradually upgrade other sources to the be shared as well in the future.

`ALTER SOURCE [ADD COLUMN | REFRESH SCHEMA]` for shared source is available since version 2.2.
</Note>

### Configure

Shared source is enabled by default. You can also set the session variable `streaming_use_shared_source` to control whether to enable it.

```sql
# change the config in the current session
SET streaming_use_shared_source=[true|false];

# change the default value of the session variable in the cluster
# (the current session is not affected)
ALTER SYSTEM SET streaming_use_shared_source=[true|false];
```

To completely disable it at the cluster level, go to [`risingwave.toml`](https://github.com/risingwavelabs/risingwave/blob/main/src/config/example.toml#L146) configuration file, and set the `stream_enable_shared_source` to `false`.

### Compared with non-shared source

With non-shared sources, when using the `CREATE SOURCE` statement: 
- No streaming jobs would be instantiated. A source is just a set of metadata stored in the catalog. 
- Only when a materialized view or sink references the source, a `SourceExecutor` will be created to start the process of data ingestion.

This leads to increased resource usage and potential inconsistencies:
- Each `SourceExecutor` consumed Kafka resources independently, adding pressure to both the Kafka broker and RisingWave.
- Independent `SourceExecutor` instances could result in different consumption progress, causing temporary inconsistencies when joining materialized views.

<Frame>
  <img src="/images/non-shared-source.png"/>
</Frame>

With shared sources, when using the `CREATE SOURCE` statement:
- It will instantiate a single `SourceExecutor` immediately. 
- All materialized views referencing the same source share the `SourceExecutor`, thus the same start offset and consumption progress.
- The downstream materialized views will only forwards data from the upstream sources, instead of consuming from Kafka independently.

This improves resource utilization and consistency.

<Frame>
  <img src="/images/shared-source.png"/>
</Frame>

When creating a materialized view, RisingWave backfills historical data from Kafka. The process blocks the DDL statement until backfill completes. 

- To configure this behavior, use the [SET BACKGROUND_DDL](/sql/commands/sql-set-background-ddl) command. This is similar to the backfilling procedure when creating a materialized view on tables and materialized views.

- To monitoring backfill progress, use the [SHOW JOBS](/sql/commands/sql-show-jobs) command or check `Kafka Consumer Lag Size` in the Grafana dashboard (under `Streaming`).

<Note>Even if a source is created with `scan.startup.mode` set to `latest`, a materialized view may still backfill from the offset resolved at source creation time. To start from the current latest offset, drop and recreate the source.</Note>

<Note>If you set up a retention policy or if the external system can only be accessed once (like message queues), and the data is no longer available, any newly created materialized views wonâ€™t be able to backfill the complete historical data. This can lead to inconsistencies with earlier materialized views.</Note>


### Compared with table

A `CREATE TABLE` statement can provide similar benefits to shared sources, except that it needs to persist all consumed data.

For table with connector, downstream materialized views backfill historical data from the table instead of external sources, which may be more efficient and cause less pressure to the external system. This also gives table stronger consistency guarantee, as historical data will be ensured to be present.

Tables offer other features that enhance their utility in data ingestion workflows. See [Table with connectors](/ingestion/overview#table-with-connectors).

<Frame>
  <img src="/images/table-with-connectors.png"/>
</Frame>

## See also

<CardGroup>
  <Card
    title="Overview of data ingestion"
    icon="database"
    iconType="solid"
    href="/ingestion/overview"
  />
  <Card
    title="DROP SOURCE"
    icon="trash"
    iconType="solid"
    href="/sql/commands/sql-drop-source"
  >
    Remove a source
  </Card>
  <Card
    title="SHOW CREATE SOURCE"
    icon="eye"
    iconType="solid"
    href="/sql/commands/sql-show-create-source"
  >
    Show the SQL statement used to create a source
  </Card>
  <Card
    title="ALTER SOURCE"
    icon="pen-to-square"
    iconType="solid"
    href="/sql/commands/sql-alter-source"
  >
    Modify a source
  </Card>
</CardGroup>
