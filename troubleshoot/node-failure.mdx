---
title: "Node failure"
description: "When it comes to crafting downtime-sensitive applications, users often have some common questions:"
---

1. What happens to the RisingWave cluster when certain nodes fail?
2. Is it feasible to deploy multiple replicas of the nodes to eliminate single points of failure?
3. How can we mitigate the impact of downtime?

In this topic, we will provide answers to these questions. However, before you continue reading, we suggest that you first explore our [fault tolerance mechanism](/reference/fault-tolerance) to gain a better understanding.

---

A RisingWave cluster consists of four internal components: streaming node, serving node, Meta Node, and Compactor Node. Additionally, there is one external component: the highly available storage backend for the Meta Node. We will explain the above questions by node type.

## Streaming Nodes

When a Streaming Node fails, the Meta Node stops receiving its heartbeat message, signaling a crash. The Meta Node then instructs all components to pause operations and awaits the return of the Streaming Node.

If RisingWave operates on Kubernetes (which we recommend for production), and a streaming node pod terminates for any reason, RisingWave relies on Kubernetes to automatically initiate a new Streaming Node pod. The total downtime comprises two phases:

1. The time taken by Kubernetes to terminate the old pod and launch the new one.
2. The time for the new Streaming Node to reconnect to the Meta Node and resume paused jobs.

It's important to mention that the duration of phase one is not under RisingWave's control and may vary based on your Kubernetes instances, including your configuration and resource allocation. These are optimized by our RisingWave Cloud service.

On the other hand, phase two is expected to be relatively short, typically lasting only a few seconds. This is due to RisingWave's storage-decoupled architecture, which eliminates the need for data movement between the old and new streaming nodes. Only lightweight metadata-level operations take place during this phase.

Based on our experience, we expect the total downtime to generally be around 20 seconds. However, with the right configuration and optimizations, it is possible to further reduce phase one, resulting in single-digit seconds of total downtime.

## Meta Nodes

If a Meta Node fails, the cluster stops all operations until the Meta Node is restored. The new Meta Node retrieves important metadata from the high-availability storage backend and reactivates the cluster.

Given the time taken for Kubernetes to create a new pod, the recommended practice for production is to have multiple replicas of the Meta Node. In the event of the current active leader's failure, one of the followers assumes leadership and restores the cluster. We expect this approach to keep the downtime at a relatively low-to-medium single-digit seconds.

## Serving Nodes

When a serving node fails, it does not affect stream processing jobs. However, if the failed serving node is the only one in the cluster, it becomes impossible to issue SQL statements.

To enhance high availability, it is recommended to provision multiple serving nodes. This allows them to operate independently, ensuring that even if one serving node goes down, the others can handle incoming SQL queries effectively.

## Compactor Nodes

When a Compactor Node fails, it does not have an immediate impact on stream processing jobs or batch queries. However, as Compute Nodes keep writing new files into RisingWave's storage engine, the read/write performance gradually decreases. This can result in throttling of input sources.

Since compaction is an append-only operation and does not modify files in place, Compactor Node crashes are rare and relatively insignificant. As long as Kubernetes restores the Compactor Node within the expected timeframe of 1 minute, it is not a major cause for concern.

## Highly-available metadata storage backends

RisingWave supports two types of metadata storage backends: etcd and relational databases (Postgres by default).

etcd is designed to be a highly available and consistent key-value storage solution. However, after equipping etcd in the production environment for a while, we learned that etcd can be quite demanding for the quality of the disk it operates on.

Therefore, we have decided to make RDS the default metadata storage backend starting from version v1.9.0 of RisingWave. Over time, we will gradually deprecate the support for etcd. This decision is based on the following factors:

* RDS (Postgres in particular) is more mature regarding its performance, reliability, and transaction support.
* Major cloud vendors offer highly available RDS offerings. RDS is a more widely accepted choice.
* There are years of best practices available for operating RDS, which can be beneficial for users deploying RisingWave in their on-prem cluster.

---

We understand that terms like "zero downtime" and "high availability" are commonly used by various cloud service vendors. However, their actual meanings can differ significantly based on different assumptions and approaches.

To ensure that your specific requirements for "zero downtime" and "high availability" are met, we encourage you to join our [Slack channel](https://www.risingwave.com/slack) and reach out to our [support team](mailto:cloud-support@risingwave-labs.com). By discussing your needs in detail, we can customize the RisingWave Cloud service to suit your specific requirements.
