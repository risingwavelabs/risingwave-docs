---
title: "Quick start"
description: "This guide aims to provide a quick and easy way to get started with RisingWave."

---

## Step 1: Start RisingWave

The following options start RisingWave in the standalone mode. In this mode, data is stored in the file system and the metadata is stored in the embedded SQLite database. See [RisingWave standalone mode](/operate/rw-standalone-mode) for more details.

For extensive testing or single-machine deployment, consider [starting RisingWave via Docker Compose](/deploy/risingwave-docker-compose). For production environments, consider [RisingWave Cloud](/deploy/risingwave-cloud), our fully managed service, or [deployment on Kubernetes using the Operator](/deploy/risingwave-kubernetes) or [Helm Chart](/deploy/risingwave-k8s-helm).

### Script installation

Open a terminal and run the following `curl` command.

```bash
curl -L https://risingwave.com/sh | sh
```

To start a RisingWave instance, run the following command.

```bash
risingwave
```

### Docker

Ensure [Docker Desktop](https://docs.docker.com/get-docker/) is installed and running in your environment.

```bash
docker run -it --pull=always -p 4566:4566 -p 5691:5691 risingwavelabs/risingwave:latest single_node
```

Ensure your hardware supports the required SIMD extensions (AVX2 for x86_64, NEON for ARM64). See [SIMD requirements for Docker images](/deploy/hardware-requirements#supported-architectures) for details.

### Homebrew

Ensure [Homebrew](https://brew.sh/) is installed, and run the following commands:

```bash
brew tap risingwavelabs/risingwave
brew install risingwave
risingwave
```

## Step 2: Connect to RisingWave

Ensure you have `psql` installed in your environment. To learn about how to install it, see [Install psql without PostgreSQL](/deploy/install-psql-without-postgresql).

Open a new terminal window and run:

```bash
psql -h localhost -p 4566 -d dev -U root
```

**Alternative: Use RisingWave Console**

Instead of using the command line, you can also use the [RisingWave Console](/risingwave-console/introduction), a web-based interface for managing and querying your RisingWave cluster. The console provides a visual way to run SQL queries, monitor cluster health, explore database schemas, and execute operational commands through an intuitive web interface.

## Step 3: Insert some data

RisingWave supports both direct data insertion and streaming data ingestion from sources like message queues and database change streams.

To keep things simple, we'll demonstrate the approach of direct data insertion. Let's create a table and insert some data.

For instance, we can create a table named `credit_card_transactions` to store information about credit card usage.

```sql Create the table
CREATE TABLE credit_card_transactions (
  card_id VARCHAR,
  amount DECIMAL,
  ts TIMESTAMP
);
```

```sql Insert five rows of data
INSERT INTO credit_card_transactions (card_id, amount, ts)
VALUES
  ('card_123', 1200.00, '2022-01-01 10:00:00'),
  ('card_123', 1800.00, '2022-01-01 10:00:20'),
  ('card_123', 1900.00, '2022-01-01 10:00:40'),
  ('card_456', 4000.00, '2022-01-01 10:01:00'),
  ('card_456', 950.00,  '2022-01-01 10:01:30');
```
## Step 4: Analyze and query data

Next, we will detect potentially fraudulent behavior by identifying any card that spends more than $5000 within a one-minute window.

To do this, we'll use a **materialized view**. A materialized view in RisingWave is not a static snapshot or a one-time query. Instead, it's a continuously maintained result that automatically stays up to date as new data arrives. You can think of it as a live dashboard behind a SQL query.

```sql Create a materialized view that detects high-spending cards in 1-minute windows
CREATE MATERIALIZED VIEW fraud_alerts AS
SELECT
  card_id,
  window_start,
  window_end,
  SUM(amount) AS total_amount
FROM
  TUMBLE(
    credit_card_transactions,  -- the source table
    ts,                        -- the event timestamp column
    INTERVAL '1 minute'        -- window size
  )
GROUP BY
  card_id, window_start, window_end
HAVING
  SUM(amount) > 5000;          -- only alert if total spend exceeds $5000
```

When this materialized view (MV) is created, RisingWave starts tracking it **immediately**. You don't need to refresh or re-run the query manually. Any future insertion into `credit_card_transactions` will automatically update `fraud_alerts`.

```sql Query the current result
SELECT * FROM fraud_alerts;
------
 card_id  |     window_start     |     window_end       | total_amount
----------+----------------------+----------------------+--------------
(0 rows)
```

At this point, no fraud alert is triggered yet. Let's insert one more transaction to push `card_123` over the \$5000 threshold.

```sql Insert additional data to trigger fraud alert
INSERT INTO credit_card_transactions (card_id, amount, ts)
VALUES ('card_123', 600.00, '2022-01-01 10:00:50');
```

```sql Query the updated result
SELECT * FROM fraud_alerts;
------
 card_id  |     window_start     |     window_end       | total_amount
----------+----------------------+----------------------+--------------
 card_123 | 2022-01-01 10:00:00  | 2022-01-01 10:01:00  | 5500.00
(1 row)
```

As you can see, the MV is automatically updated when new data is inserted. You don't need to manage any refresh logic or manual updates. RisingWave handles the incremental computation for you in the background.

## What's next?

Congratulations! You've successfully started RisingWave and conducted some initial data analysis. To explore further, you may want to:

* Check out the ready-to-run examples:
   * [Example A: Ingest data from Kafka](https://github.com/risingwavelabs/awesome-stream-processing/blob/main/00-get-started/01-ingest-kafka-data.md)
   * [Example B: Ingest data from Postgres CDC](https://github.com/risingwavelabs/awesome-stream-processing/blob/main/00-get-started/02-ingest-pg-cdc.md)
* See [this GitHub directory](https://github.com/risingwavelabs/risingwave/tree/main/integration%5Ftests) for ready-to-run demos and integration examples.
* Read our documentation to learn about how to ingest data from data streaming sources, transform data, and deliver data to downstream systems.
