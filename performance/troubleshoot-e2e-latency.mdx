---
title: "End-to-End latency"
description: ""
---

Streaming applications often prioritize real-time insights, making end-to-end (E2E) latency a critical factor for success. In RisingWave, E2E latency refers to the total time taken for a data event to be ingested, processed, and delivered to the final destination. High latency can significantly impact real-time use cases such as financial transactions, anomaly detection, and real-time analytics, where even milliseconds matter.

## What is E2E latency in RisingWave?

E2E latency in RisingWave consists of multiple stages:

- Source latency: The time taken to ingest new data from upstream sources like Kafka, databases, or object storage.
Processing Latency: The time RisingWave takes to transform and compute results on the ingested data.
Sink Latency: The time taken to write computed results to a target system (e.g., Kafka, PostgreSQL, or MySQL).
Each of these stages contributes to the overall E2E latency, so optimizing them collectively is essential to achieving low latency.

Materialized Views vs. Sink Latency Considerations
Notice that we have discussed E2E latency in terms of sinks but not materialized views. This is because updates to a materialized view are not immediately available for batch queries.

RisingWave enforces strong consistency for materialized views, meaning updates must wait for the nearest barrier to be globally committed. The commit interval is controlled by the barrier_interval_ms system parameter.

In contrast, sink outputs are written eagerly to the downstream system without waiting for barriers to commit.

If an application requires latency ≤ 500ms, we generally recommend using a sink to trigger event-driven business logic downstream, rather than relying on a built-in materialized view with periodic polling, to ensure faster data availability.

Typically, when users aim for very low E2E latency, they use a message queue like Kafka as both the source and sink. Based on our experience, when properly configured, source and sink latency can be as low as a few milliseconds 1~5ms.

As a result, the primary factor influencing E2E latency is processing latency within RisingWave. This depends on several key aspects:

Query Complexity and Data Patterns
The simpler the query and data flow, the lower the processing latency. The computational overhead is largely determined by:

The number and type of operators (e.g., joins, aggregations).
The volume of intermediate results generated, such as join amplification.
Even with optimal configurations, if a query requires significant computation, achieving ultra-low latency may not be feasible. Therefore, if your query is computationally intensive—either due to complexity or data volume—consider rewriting the query or adjusting the business goal to better align with the desired latency constraints.

Tradeoff between lower latency and higher throughput
https://blog.danslimmon.com/2019/02/26/the-latency-throughput-tradeoff-why-fast-services-are-slow-and-vice-versa/.
There is an inherent trade-off between lower latency and higher throughput. Achieving ultra-low latency often requires processing events as soon as they arrive, which can limit batch optimizations and reduce overall system efficiency. Conversely, higher throughput is typically achieved by batching data, reducing per-event overhead but introducing additional delays.

One notable configuration parameter in the aggregation operator is stream_hash_agg_max_dirty_groups_heap_size ([source](https://github.com/risingwavelabs/risingwave/blob/0bedf6503e7ceb50d40637083fc34bb735f0a0aa/src/config/example.toml#L153)).

This parameter controls how many changes the aggregation operator buffers before consolidating them and sending the results to the next operator in the query plan. The trade-off here is between latency and computation efficiency.

Example: Trade-Off Between Consolidation and Immediate Output
Consider a scenario where we receive a series of updates for key K:

(K, -1) → (K, +1) → (K, -1) → (K, +1)

If the aggregation operator buffers and consolidates these updates before emitting them downstream, the net effect is zero change—no output needs to be sent.

However, if we immediately propagate each individual change as it arrives, the system outputs four updates. This can have significant downstream effects, especially if multiple join operators follow, potentially leading to a large number of intermediate results.

Key Considerations for Performance Tuning
Lower values of stream_hash_agg_max_dirty_groups_heap_size prioritize low latency, ensuring changes are sent downstream as soon as they arrive. However, the second-order effect is that it can increase the load on downstream operators, potentially slowing them down due to a higher volume of intermediate results.
Higher values buffer more updates before consolidating them, which reduces redundant computations and minimizes intermediate results—at the cost of slightly increased latency.
When tuning this parameter, users should consider the overall query structure and whether downstream operations, such as joins, may amplify unnecessary updates if changes are emitted too frequently.

Another trade-off between lower latency and higher throughput lies in the parallelism configuration for each streaming job.

To achieve higher throughput, it is natural to increase parallelism and execute the job in a distributed fashion. However, higher parallelism does not always lead to lower latency, as it introduces additional overhead.

For example, in a hash join, data from one upstream source must be shuffled to the corresponding hash partitions before being joined with data from the other upstream. If there are many upstream sources, this shuffling stage can add more latency.

The impact becomes even more pronounced when a query involves multiple shuffle stages, such as hash joins or aggregations on different keys, where each stage introduces additional delays.

That said, if a query can be executed in an embarrassingly parallel manner—where tasks run independently without shuffling—this concern does not apply.

Expanding on this idea, it is often more efficient to use a single large physical machine for computation rather than multiple smaller machines, as this helps minimize shuffling overhead and reduce network latency.

Finally, if you experience higher-than-expected latency, refer to this troubleshooting guide for initial debugging steps. If the issue persists, join our Slack community, and we can help diagnose it with more details.

